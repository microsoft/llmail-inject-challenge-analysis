{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee099311",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json  \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a840051f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/disk1/miniconda3/envs/satml_comp/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "## get data first\n",
    "from datasets import load_dataset\n",
    "submissions = load_dataset(\"microsoft/llmail-inject-challenge\")['Phase1']\n",
    "jobs = [submissions._getitem(i) for i in range(len(submissions))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd916ef4",
   "metadata": {},
   "source": [
    "### For which retrieval level?  (leave empty if for all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "520cd010",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_level = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11249a92",
   "metadata": {},
   "source": [
    "## For each team: 1) check the solved levels and their times (first solve)\n",
    "\n",
    "## Also check for each level which teams solved it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "65730f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "#### all jobs that solved a level per team ####\n",
    "teams_to_solves = dict()\n",
    "\n",
    "#### map levels to teams whol solves it \n",
    "levels_to_team_solved = dict()\n",
    "\n",
    "for job in jobs:\n",
    "    if job['output']:\n",
    "        obj = json.loads(job['objectives']) \n",
    "        if obj:\n",
    "            ## successful job \n",
    "            if all(obj.values()):\n",
    "                if not job['team_id'] in teams_to_solves: teams_to_solves[job['team_id']] = dict()\n",
    "                if not job['scenario'] in teams_to_solves[job['team_id']]: teams_to_solves[job['team_id']][job['scenario']] = []\n",
    "                teams_to_solves[job['team_id']][job['scenario']].append(datetime.strptime(job['completed_time'],\"%Y-%m-%dT%H:%M:%S.%f%z\"))\n",
    "                \n",
    "                if not job['scenario'] in levels_to_team_solved: levels_to_team_solved[job['scenario']] = set()\n",
    "                levels_to_team_solved[job['scenario']].add(job['team_id'])\n",
    "                \n",
    "\n",
    "\n",
    "#### first solve for each team in each level ####\n",
    "teams_to_first_solve = dict()\n",
    "for team in teams_to_solves:\n",
    "    teams_to_first_solve[team] = dict()\n",
    "    for level in teams_to_solves[team]:\n",
    "        first_solve = min(teams_to_solves[team][level])\n",
    "        teams_to_first_solve[team][level] = first_solve\n",
    "\n",
    "\n",
    "#### initialize attempts before solve ####\n",
    "teams_to_attempts_before_solve = dict()\n",
    "for team in teams_to_solves:\n",
    "    teams_to_attempts_before_solve[team] = dict()\n",
    "    for level in teams_to_solves[team]:\n",
    "        teams_to_attempts_before_solve[team][level] = 0 \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7287a36",
   "metadata": {},
   "source": [
    "## For each job: \n",
    "### map it to which team submitted it \n",
    "### check if that team has solved it\n",
    "#### if yes, check if the job time is less than the solve time\n",
    "##### if yes, increment the trials before solve for that team and that level. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e9a09a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "for job in jobs: \n",
    "    obj = json.loads(job['objectives'])\n",
    "    if not obj: continue\n",
    "    team_id = job['team_id']\n",
    "    scenario = job['scenario']\n",
    "    job_completed_time = datetime.strptime(job['completed_time'],\"%Y-%m-%dT%H:%M:%S.%f%z\")\n",
    "    \n",
    "    #scenario ever solved?\n",
    "    if scenario in levels_to_team_solved:\n",
    "        #team solved it?\n",
    "        if team_id in levels_to_team_solved[scenario]:\n",
    "            #less than the first solve? \n",
    "            if job_completed_time <  teams_to_first_solve[team_id][scenario]:\n",
    "                    teams_to_attempts_before_solve[team_id][scenario] += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4ace48",
   "metadata": {},
   "source": [
    "## for each defense:\n",
    "### get a list of trials_before_success for each team who solved it and average it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "61c48e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt shield - mean: 88.00611620795107, std: 426.2277965116387\n",
      "task tracker - mean: 158.31336405529953, std: 518.5892627715648\n",
      "llm judge - mean: 505.23595505617976, std: 948.4642823464726\n"
     ]
    }
   ],
   "source": [
    "trials_before_success_per_defense = {'prompt_shield': [], 'task_tracker': [], 'llm_judge': []}\n",
    "\n",
    "prompt_shield = ['a', 'b']\n",
    "tasktracker = ['c', 'd']\n",
    "llm_judge = ['g', 'h']\n",
    "\n",
    "def check_defense(scenario, defense_letters): \n",
    "    for letter in defense_letters: \n",
    "        if letter in scenario: return True \n",
    "    return False \n",
    "\n",
    "\n",
    "for scenario in levels_to_team_solved:\n",
    "    for team in teams_to_attempts_before_solve:\n",
    "        if scenario in teams_to_attempts_before_solve[team]:\n",
    "            trials_before_succes_for_team = teams_to_attempts_before_solve[team][scenario]\n",
    "            if not retrieval_level in scenario: continue \n",
    "            scenario_  = scenario.split('level')[-1]\n",
    "            if check_defense(scenario_,prompt_shield):\n",
    "                trials_before_success_per_defense['prompt_shield'].append(trials_before_succes_for_team)\n",
    "            elif check_defense(scenario_,tasktracker):\n",
    "                trials_before_success_per_defense['task_tracker'].append(trials_before_succes_for_team)\n",
    "            elif check_defense(scenario_,llm_judge):\n",
    "                trials_before_success_per_defense['llm_judge'].append(trials_before_succes_for_team)\n",
    "\n",
    "import numpy as np \n",
    "\n",
    "print(f'prompt shield - mean: {np.mean(trials_before_success_per_defense['prompt_shield'])}, std: {np.std(trials_before_success_per_defense['prompt_shield'])}')\n",
    "\n",
    "\n",
    "print(f'task tracker - mean: {np.mean(trials_before_success_per_defense['task_tracker'])}, std: {np.std(trials_before_success_per_defense['task_tracker'])}')\n",
    "\n",
    "\n",
    "print(f'llm judge - mean: {np.mean(trials_before_success_per_defense['llm_judge'])}, std: {np.std(trials_before_success_per_defense['llm_judge'])}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "satml_comp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
